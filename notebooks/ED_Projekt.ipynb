{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ciyx4-A22Hu9"
      },
      "source": [
        "Projekt zaliczeniowy na przedmiot Eksploracja Danych\n",
        "\n",
        "Analiza skupień państw i narodów w kontekście gospodarczo-politycznym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alon4zIL2Es-"
      },
      "source": [
        "## 1. Importowanie bibliotek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXyO6oW71Iuo"
      },
      "outputs": [],
      "source": [
        "# Importowanie bibliotek\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer, MinMaxScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import silhouette_score, normalized_mutual_info_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONK49u7NI91a"
      },
      "source": [
        "## 2. Funkcje - wczytanie danych i wykrey\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1KQBuOjI91c"
      },
      "outputs": [],
      "source": [
        "def load_and_clean_data(data_files, index_file_path='../data/index.csv', max_nan_percent=0.5):\n",
        "    try:\n",
        "        index_df = pd.read_csv(index_file_path, keep_default_na=False, na_values=[])\n",
        "        if 'iso_3166_1_alpha_3' not in index_df.columns or 'location_key' not in index_df.columns:\n",
        "            raise KeyError(\"Index file must contain 'iso_3166_1_alpha_3' and 'location_key' columns.\")\n",
        "        alpha3_to_alpha2_map = dict(zip(index_df['iso_3166_1_alpha_3'], index_df['location_key']))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Index file '{index_file_path}' not found!\")\n",
        "        return None, None\n",
        "\n",
        "    raw_data = {}\n",
        "    for key, file in data_files.items():\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df.replace(\"..\", np.nan, inplace=True)\n",
        "            df['Country Code'] = df['Country Code'].map(alpha3_to_alpha2_map)\n",
        "            df = df[df['Country Code'].notna()]\n",
        "            df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            if \"Country Code\" in df.columns:\n",
        "                df[\"location_key\"] = df[\"Country Code\"]\n",
        "                df.drop(columns=[\"Country Code\"], inplace=True)\n",
        "\n",
        "            if \"Country Name\" in df.columns:\n",
        "                df.rename(columns={\"Country Name\": \"country_name\"}, inplace=True)\n",
        "\n",
        "            raw_data[key] = df\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Data file '{file}' not found!\")\n",
        "            return None, None\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file '{file}': {e}\")\n",
        "            return None, None\n",
        "\n",
        "    common_keys = set(index_df[\"location_key\"].dropna().unique())\n",
        "    for df in raw_data.values():\n",
        "        if \"location_key\" in df.columns:\n",
        "            common_keys &= set(df[\"location_key\"].dropna().unique())\n",
        "\n",
        "    cleaned_dfs = {}\n",
        "    for key, df in raw_data.items():\n",
        "        if \"location_key\" not in df.columns:\n",
        "            print(f\"Error: {key} missing location_key!\")\n",
        "            continue\n",
        "\n",
        "        df = df[df[\"location_key\"].isin(common_keys)].copy()\n",
        "        max_nans = int(max_nan_percent * len(df))\n",
        "        nan_counts = df.isna().sum()\n",
        "        dropped_cols = nan_counts[nan_counts > max_nans].index.tolist()\n",
        "        #if dropped_cols:\n",
        "            #print(f\"Dropped columns for {key} (over {max_nan_percent*100}% NaN): {dropped_cols}\")\n",
        "        df = df.loc[:, nan_counts <= max_nans]\n",
        "\n",
        "        numeric_cols = [col for col in df.columns if col not in [\"location_key\", \"country_name\"]]\n",
        "        for col in numeric_cols:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "            if df[col].notna().sum() > 0:\n",
        "                df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        cleaned_dfs[key] = df\n",
        "\n",
        "    e_ix = index_df[index_df[\"location_key\"].isin(common_keys)]\n",
        "    return cleaned_dfs, e_ix\n",
        "\n",
        "def plot_kbins_visualization(df_no_key, silhouette_scores, n_bins_range, dataset_name):\n",
        "    # plt.figure(figsize=(12, 5))\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    # numeric_cols = df_no_key.select_dtypes(include=np.number).columns\n",
        "    # if not numeric_cols.empty:\n",
        "    #     feature = numeric_cols[0]\n",
        "    #     plt.hist(df_no_key[feature], bins=5, edgecolor='black', color='skyblue')\n",
        "    #     plt.xlabel(f\"Feature Value ({feature})\")\n",
        "    #     plt.ylabel(\"Number of Countries\")\n",
        "    #     plt.title(f\"K-bins Histogram for {dataset_name}\")\n",
        "    # else:\n",
        "    #     plt.title(f\"No numeric features for K-bins Histogram in {dataset_name}\")\n",
        "    #     plt.axis('off')\n",
        "\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(n_bins_range, silhouette_scores, marker='o', color='navy')\n",
        "    if silhouette_scores:\n",
        "        max_score_idx = silhouette_scores.index(max(silhouette_scores))\n",
        "        plt.scatter([n_bins_range[max_score_idx]],\n",
        "                    [silhouette_scores[max_score_idx]], color='red', s=100, label='Optimal Bins')\n",
        "        plt.legend()\n",
        "    plt.xlabel(\"Number of Bins\")\n",
        "    plt.ylabel(\"Silhouette Score\")\n",
        "    plt.title(f\"Optimal Number of Bins for {dataset_name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_svd_visualization(data_scaled, dataset_name):\n",
        "    svd = TruncatedSVD(n_components=min(10, data_scaled.shape[1] - 1))\n",
        "    if data_scaled.shape[1] == 0:\n",
        "        print(f\"Warning: No features in {dataset_name} for SVD. Skipping plot.\")\n",
        "        return svd\n",
        "    svd.fit(data_scaled)\n",
        "    explained_variance = svd.explained_variance_ratio_\n",
        "    cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
        "    plt.axhline(y=0.9, color='red', linestyle='--', label='90% Explained Variance')\n",
        "    plt.xlabel(\"Number of Components\")\n",
        "    plt.ylabel(\"Cumulative Explained Variance\")\n",
        "    plt.title(f\"SVD Explained Variance (Cumulative) for {dataset_name}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    return svd\n",
        "\n",
        "\n",
        "\n",
        "def plot_dbscan_interactive(data_2d, clusters, location_keys, e_ix, dataset_name):\n",
        "    loc_to_name = dict(zip(e_ix['location_key'], e_ix['country_name']))\n",
        "    countries = [loc_to_name.get(lk, lk) for lk in location_keys]\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    unique_clusters = sorted(set(clusters))\n",
        "    colors = px.colors.qualitative.Dark24\n",
        "\n",
        "    for i, cluster_id in enumerate(unique_clusters):\n",
        "        cluster_data_indices = np.where(clusters == cluster_id)[0]\n",
        "        cluster_x = data_2d[cluster_data_indices, 0]\n",
        "        cluster_y = data_2d[cluster_data_indices, 1]\n",
        "        cluster_countries = [countries[j] for j in cluster_data_indices]\n",
        "        label = \"Noise\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=cluster_x,\n",
        "            y=cluster_y,\n",
        "            mode='markers',\n",
        "            name=label,\n",
        "            text=cluster_countries,\n",
        "            hoverinfo='text',\n",
        "            marker=dict(\n",
        "                size=12,\n",
        "                color=colors[i % len(colors)] if cluster_id != -1 else 'white',\n",
        "                line=dict(width=1, color='DarkSlateGrey')\n",
        "            )\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f'DBSCAN Cluster Visualization for {dataset_name}',\n",
        "        xaxis=dict(showgrid=False, zeroline=False, title='Component 1'),\n",
        "        yaxis=dict(showgrid=False, zeroline=False, title='Component 2'),\n",
        "        height=700,\n",
        "        width=1000,\n",
        "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
        "        clickmode='event+select'\n",
        "    )\n",
        "\n",
        "    fig.add_annotation(\n",
        "        x=0.01,\n",
        "        y=0.01,\n",
        "        xref=\"paper\",\n",
        "        yref=\"paper\",\n",
        "        text=f\"Total points: {len(data_2d)}, Noise points: {(clusters == -1).sum()}\",\n",
        "        showarrow=False,\n",
        "        font=dict(size=10, color=\"gray\")\n",
        "    )\n",
        "\n",
        "    country_selector = widgets.Select(\n",
        "        options=['None'] + sorted(countries),\n",
        "        value='None',\n",
        "        description='Select Country:',\n",
        "        rows=15\n",
        "    )\n",
        "\n",
        "    output_plot = widgets.Output()\n",
        "\n",
        "    def on_country_select(change):\n",
        "        with output_plot:\n",
        "            clear_output(wait=True)\n",
        "            selected_country = change.new\n",
        "            updated_fig = go.Figure(fig)\n",
        "\n",
        "            for trace_idx, trace in enumerate(updated_fig.data):\n",
        "                cluster_indices = np.where(clusters == unique_clusters[trace_idx])[0]\n",
        "                original_colors = [colors[clusters[j] % len(colors)] if clusters[j] != -1 else 'black' for j in cluster_indices]\n",
        "                updated_fig.data[trace_idx].marker.size = [12] * len(cluster_indices)\n",
        "                updated_fig.data[trace_idx].marker.color = original_colors\n",
        "\n",
        "            updated_fig.update_layout(annotations=[dict(\n",
        "                x=0.01, y=0.01, xref=\"paper\", yref=\"paper\",\n",
        "                text=f\"Total points: {len(data_2d)}, Noise points: {(clusters == -1).sum()}\",\n",
        "                showarrow=False, font=dict(size=10, color=\"gray\")\n",
        "            )])\n",
        "\n",
        "            if selected_country != 'None':\n",
        "                found_idx = countries.index(selected_country) if selected_country in countries else -1\n",
        "                if found_idx != -1:\n",
        "                    selected_cluster_id = clusters[found_idx]\n",
        "                    selected_x = data_2d[found_idx, 0]\n",
        "                    selected_y = data_2d[found_idx, 1]\n",
        "\n",
        "                    trace_idx = unique_clusters.index(selected_cluster_id)\n",
        "                    cluster_countries = list(updated_fig.data[trace_idx].text)\n",
        "                    idx_in_trace = cluster_countries.index(selected_country)\n",
        "                    current_sizes = list(updated_fig.data[trace_idx].marker.size)\n",
        "                    current_colors = list(updated_fig.data[trace_idx].marker.color)\n",
        "\n",
        "                    current_sizes[idx_in_trace] = 20\n",
        "                    current_colors[idx_in_trace] = 'red'\n",
        "                    updated_fig.data[trace_idx].marker.size = current_sizes\n",
        "                    updated_fig.data[trace_idx].marker.color = current_colors\n",
        "\n",
        "                    updated_fig.add_annotation(\n",
        "                        x=selected_x, y=selected_y, text=selected_country,\n",
        "                        showarrow=True, arrowhead=1, ax=20, ay=-30,\n",
        "                        font=dict(size=12, color=\"black\"),\n",
        "                        bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
        "                        bordercolor=\"black\", borderwidth=1\n",
        "                    )\n",
        "\n",
        "            display(updated_fig)\n",
        "\n",
        "    def on_plot_click(data):\n",
        "        with output_plot:\n",
        "            clear_output(wait=True)\n",
        "            country_selector.value = 'None'\n",
        "            updated_fig = go.Figure(fig)\n",
        "\n",
        "            for trace_idx, trace in enumerate(updated_fig.data):\n",
        "                cluster_indices = np.where(clusters == unique_clusters[trace_idx])[0]\n",
        "                original_colors = [colors[clusters[j] % len(colors)] if clusters[j] != -1 else 'black' for j in cluster_indices]\n",
        "                updated_fig.data[trace_idx].marker.size = [12] * len(cluster_indices)\n",
        "                updated_fig.data[trace_idx].marker.color = original_colors\n",
        "\n",
        "            updated_fig.update_layout(annotations=[dict(\n",
        "                x=0.01, y=0.01, xref=\"paper\", yref=\"paper\",\n",
        "                text=f\"Total points: {len(data_2d)}, Noise points: {(clusters == -1).sum()}\",\n",
        "                showarrow=False, font=dict(size=10, color=\"gray\")\n",
        "            )])\n",
        "            display(updated_fig)\n",
        "\n",
        "    country_selector.observe(on_country_select, names='value')\n",
        "    if fig.data:\n",
        "        fig.data[0].on_click(on_plot_click)\n",
        "\n",
        "    dashboard = widgets.HBox([country_selector, output_plot])\n",
        "\n",
        "    with output_plot:\n",
        "        display(fig)\n",
        "\n",
        "    display(dashboard)\n",
        "\n",
        "def plot_interactive_clusters_graph(data_2d, labels, location_keys, e_ix, dataset_name):\n",
        "    loc_to_name = dict(zip(e_ix['location_key'], e_ix['country_name']))\n",
        "    countries = [loc_to_name.get(lk, lk) for lk in location_keys]\n",
        "\n",
        "    cluster_centers = {}\n",
        "    for cluster_id in set(labels):\n",
        "        indices = [i for i, lbl in enumerate(labels) if lbl == cluster_id]\n",
        "        cluster_points = data_2d[indices]\n",
        "        if len(cluster_points) > 0:\n",
        "            cluster_centers[cluster_id] = np.mean(cluster_points, axis=0)\n",
        "        else:\n",
        "            cluster_centers[cluster_id] = np.array([0, 0])\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    unique_clusters = sorted(set(labels))\n",
        "    colors = px.colors.qualitative.Dark24\n",
        "\n",
        "    for i, cluster_id in enumerate(unique_clusters):\n",
        "        cluster_data_indices = np.where(labels == cluster_id)[0]\n",
        "        cluster_x = data_2d[cluster_data_indices, 0]\n",
        "        cluster_y = data_2d[cluster_data_indices, 1]\n",
        "        cluster_countries = [countries[j] for j in cluster_data_indices]\n",
        "\n",
        "        edge_x, edge_y = [], []\n",
        "        for idx in cluster_data_indices:\n",
        "            point = data_2d[idx]\n",
        "            center = cluster_centers[cluster_id]\n",
        "            edge_x += [point[0], center[0], None]\n",
        "            edge_y += [point[1], center[1], None]\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=edge_x, y=edge_y,\n",
        "            line=dict(width=0.5, color='#888'),\n",
        "            hoverinfo='none',\n",
        "            mode='lines',\n",
        "            name=f'Cluster {cluster_id}',\n",
        "            legendgroup=f'group_{cluster_id}',\n",
        "            showlegend=False\n",
        "        ))\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=cluster_x,\n",
        "            y=cluster_y,\n",
        "            mode='markers',\n",
        "            name=f'Cluster {cluster_id}',\n",
        "            text=cluster_countries,\n",
        "            hoverinfo='text',\n",
        "            marker=dict(\n",
        "                size=12,\n",
        "                color=colors[i % len(colors)],\n",
        "                line=dict(width=1, color='DarkSlateGrey')\n",
        "            ),\n",
        "            legendgroup=f'group_{cluster_id}'\n",
        "        ))\n",
        "\n",
        "        if cluster_id in cluster_centers:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[cluster_centers[cluster_id][0]],\n",
        "                y=[cluster_centers[cluster_id][1]],\n",
        "                mode='markers',\n",
        "                name=f'Centroid {cluster_id}',\n",
        "                text=[f\"Centroid {cluster_id}\"],\n",
        "                hoverinfo='text',\n",
        "                marker=dict(\n",
        "                    size=15,\n",
        "                    color='black',\n",
        "                    symbol='diamond',\n",
        "                    line=dict(width=2, color='white')\n",
        "                ),\n",
        "                legendgroup=f'group_{cluster_id}',\n",
        "                showlegend=False\n",
        "            ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f'Cluster Visualization for {dataset_name}',\n",
        "        xaxis=dict(showgrid=False, zeroline=False, title='Component 1'),\n",
        "        yaxis=dict(showgrid=False, zeroline=False, title='Component 2'),\n",
        "        height=700,\n",
        "        width=1000,\n",
        "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
        "        clickmode='event+select'\n",
        "    )\n",
        "\n",
        "    fig.add_annotation(\n",
        "        x=0.01,\n",
        "        y=0.01,\n",
        "        xref=\"paper\",\n",
        "        yref=\"paper\",\n",
        "        text=f\"Total points: {len(data_2d)}\",\n",
        "        showarrow=False,\n",
        "        font=dict(size=10, color=\"gray\")\n",
        "    )\n",
        "\n",
        "    country_selector = widgets.Select(\n",
        "        options=['None'] + sorted(countries),\n",
        "        value='None',\n",
        "        description='Select Country:',\n",
        "        rows=15\n",
        "    )\n",
        "\n",
        "    output_plot = widgets.Output()\n",
        "\n",
        "    def on_country_select(change):\n",
        "        with output_plot:\n",
        "            clear_output(wait=True)\n",
        "            selected_country = change.new\n",
        "            updated_fig = go.Figure(fig)\n",
        "\n",
        "            for trace_idx, trace in enumerate(updated_fig.data):\n",
        "                if trace.mode == 'markers' and trace.marker.symbol != 'diamond':\n",
        "                    cluster_id = unique_clusters[trace_idx // 3]\n",
        "                    cluster_indices = np.where(labels == cluster_id)[0]\n",
        "                    original_colors = [colors[labels[j] % len(colors)] for j in cluster_indices]\n",
        "                    updated_fig.data[trace_idx].marker.size = [12] * len(cluster_indices)\n",
        "                    updated_fig.data[trace_idx].marker.color = original_colors\n",
        "\n",
        "            updated_fig.update_layout(annotations=[dict(\n",
        "                x=0.01, y=0.01, xref=\"paper\", yref=\"paper\",\n",
        "                text=f\"Total points: {len(data_2d)}\",\n",
        "                showarrow=False, font=dict(size=10, color=\"gray\")\n",
        "            )])\n",
        "\n",
        "            if selected_country != 'None':\n",
        "                found_idx = countries.index(selected_country) if selected_country in countries else -1\n",
        "                if found_idx != -1:\n",
        "                    selected_cluster_id = labels[found_idx]\n",
        "                    selected_x = data_2d[found_idx, 0]\n",
        "                    selected_y = data_2d[found_idx, 1]\n",
        "\n",
        "                    trace_idx = unique_clusters.index(selected_cluster_id) * 3 + 1\n",
        "                    cluster_countries = list(updated_fig.data[trace_idx].text)\n",
        "                    idx_in_trace = cluster_countries.index(selected_country)\n",
        "                    current_sizes = list(updated_fig.data[trace_idx].marker.size)\n",
        "                    current_colors = list(updated_fig.data[trace_idx].marker.color)\n",
        "\n",
        "                    current_sizes[idx_in_trace] = 20\n",
        "                    current_colors[idx_in_trace] = 'red'\n",
        "                    updated_fig.data[trace_idx].marker.size = current_sizes\n",
        "                    updated_fig.data[trace_idx].marker.color = current_colors\n",
        "\n",
        "                    updated_fig.add_annotation(\n",
        "                        x=selected_x, y=selected_y, text=selected_country,\n",
        "                        showarrow=True, arrowhead=1, ax=20, ay=-30,\n",
        "                        font=dict(size=12, color=\"black\"),\n",
        "                        bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
        "                        bordercolor=\"black\", borderwidth=1\n",
        "                    )\n",
        "\n",
        "            display(updated_fig)\n",
        "\n",
        "    def on_plot_click(data):\n",
        "        with output_plot:\n",
        "            clear_output(wait=True)\n",
        "            country_selector.value = 'None'\n",
        "            updated_fig = go.Figure(fig)\n",
        "\n",
        "            for trace_idx, trace in enumerate(updated_fig.data):\n",
        "                if trace.mode == 'markers' and trace.marker.symbol != 'diamond':\n",
        "                    cluster_id = unique_clusters[trace_idx // 3]\n",
        "                    cluster_indices = np.where(labels == cluster_id)[0]\n",
        "                    original_colors = [colors[labels[j] % len(colors)] for j in cluster_indices]\n",
        "                    updated_fig.data[trace_idx].marker.size = [12] * len(cluster_indices)\n",
        "                    updated_fig.data[trace_idx].marker.color = original_colors\n",
        "\n",
        "            updated_fig.update_layout(annotations=[dict(\n",
        "                x=0.01, y=0.01, xref=\"paper\", yref=\"paper\",\n",
        "                text=f\"Total points: {len(data_2d)}\",\n",
        "                showarrow=False, font=dict(size=10, color=\"gray\")\n",
        "            )])\n",
        "            display(updated_fig)\n",
        "\n",
        "    country_selector.observe(on_country_select, names='value')\n",
        "    if fig.data:\n",
        "        fig.data[0].on_click(on_plot_click)\n",
        "\n",
        "    dashboard = widgets.HBox([country_selector, output_plot])\n",
        "\n",
        "    with output_plot:\n",
        "        display(fig)\n",
        "\n",
        "    display(dashboard)\n",
        "\n",
        "\n",
        "def plot_dendrogram(Z, location_keys, e_ix, dataset_name):\n",
        "    loc_to_name = dict(zip(e_ix['location_key'], e_ix['country_name']))\n",
        "    labels = [loc_to_name.get(lk, lk) for lk in location_keys]\n",
        "\n",
        "    plt.figure(figsize=(30, 15))\n",
        "\n",
        "    dendrogram(Z, labels=labels, leaf_rotation=90, leaf_font_size=12)\n",
        "    plt.title(f\"Dendrogram for {dataset_name}\")\n",
        "    plt.xlabel(\"Countries\")\n",
        "    plt.ylabel(\"Distance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcZ-CGUtI91g"
      },
      "source": [
        "Analiza eksprolracyjna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOSoZVyQI91j",
        "outputId": "8e1c8de5-ec85-48d4-e1e5-737d60163fd4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'HTML' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 226\u001b[0m\n\u001b[0;32m    212\u001b[0m         display(HTML(style_table(feature_stats)))\n\u001b[0;32m    216\u001b[0m data_files \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb_econ\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/wb_econ.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb_env\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/wb_env.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb_soc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/wb_soc.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m }\n\u001b[1;32m--> 226\u001b[0m \u001b[43mexploratory_data_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_log_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[8], line 141\u001b[0m, in \u001b[0;36mexploratory_data_analysis\u001b[1;34m(data_files, index_file_path, apply_log_transform)\u001b[0m\n\u001b[0;32m    139\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(comparison_data)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m comparison_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m--> 141\u001b[0m     display(\u001b[43mHTML\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<h2>Comparison of All Datasets (Pre- and Post-Imputation)</h2>\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    142\u001b[0m     display(HTML(comparison_df\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39mset_table_styles([\n\u001b[0;32m    143\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselector\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprops\u001b[39m\u001b[38;5;124m'\u001b[39m: [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground-color\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#4CAF50\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfont-weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-align\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8px\u001b[39m\u001b[38;5;124m'\u001b[39m)]},\n\u001b[0;32m    144\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselector\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprops\u001b[39m\u001b[38;5;124m'\u001b[39m: [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1px solid #ddd\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8px\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-align\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m)]},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselector\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr:hover\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprops\u001b[39m\u001b[38;5;124m'\u001b[39m: [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground-color\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#ddd\u001b[39m\u001b[38;5;124m'\u001b[39m)]}\n\u001b[0;32m    148\u001b[0m     ])\u001b[38;5;241m.\u001b[39mset_caption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Comparison (Pre- and Post-Imputation)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mhide(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_html()))\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Individual dataset analysis (post-imputation only)\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'HTML' is not defined"
          ]
        }
      ],
      "source": [
        "def load_and_clean_data(data_files, index_file_path='index.csv', max_nan_percent=0.5, apply_log_transform=False):\n",
        "\n",
        "    try:\n",
        "        index_df = pd.read_csv(index_file_path, keep_default_na=False, na_values=[])\n",
        "        if 'iso_3166_1_alpha_3' not in index_df.columns or 'location_key' not in index_df.columns:\n",
        "            raise KeyError(\"Index file must contain 'iso_3166_1_alpha_3' and 'location_key' columns.\")\n",
        "        alpha3_to_alpha2_map = dict(zip(index_df['iso_3166_1_alpha_3'], index_df['location_key']))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Index file '{index_file_path}' not found!\")\n",
        "        return None, None, None\n",
        "\n",
        "    raw_data = {}\n",
        "    for key, file in data_files.items():\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df.replace(\"..\", np.nan, inplace=True)\n",
        "            df['Country Code'] = df['Country Code'].map(alpha3_to_alpha2_map)\n",
        "            df = df[df['Country Code'].notna()]\n",
        "            df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            if \"Country Code\" in df.columns:\n",
        "                df[\"location_key\"] = df[\"Country Code\"]\n",
        "                df.drop(columns=[\"Country Code\"], inplace=True)\n",
        "\n",
        "            if \"Country Name\" in df.columns:\n",
        "                df.rename(columns={\"Country Name\": \"country_name\"}, inplace=True)\n",
        "\n",
        "            raw_data[key] = df\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Data file '{file}' not found!\")\n",
        "            return None, None, None\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file '{file}': {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    common_keys = set(index_df[\"location_key\"].dropna().unique())\n",
        "    for df in raw_data.values():\n",
        "        if \"location_key\" in df.columns:\n",
        "            common_keys &= set(df[\"location_key\"].dropna().unique())\n",
        "\n",
        "    pre_imputation_dfs = {}\n",
        "    post_imputation_dfs = {}\n",
        "    for key, df in raw_data.items():\n",
        "        if \"location_key\" not in df.columns:\n",
        "            print(f\"Error: {key} missing location_key!\")\n",
        "            continue\n",
        "\n",
        "        df = df[df[\"location_key\"].isin(common_keys)].copy()\n",
        "        max_nans = int(max_nan_percent * len(df))\n",
        "        nan_counts = df.isna().sum()\n",
        "        df = df.loc[:, nan_counts <= max_nans]\n",
        "\n",
        "        # Store pre-imputation DataFrame\n",
        "        pre_imputation_dfs[key] = df.copy()\n",
        "\n",
        "        # Perform imputation and numeric conversion\n",
        "        numeric_cols = [col for col in df.columns if col not in [\"location_key\", \"country_name\"]]\n",
        "        for col in numeric_cols:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "            if df[col].isna().all():\n",
        "                df.drop(columns=[col], inplace=True)  # Drop entirely NaN columns\n",
        "            elif df[col].notna().any():\n",
        "                df[col].fillna(df[col].median(), inplace=True)\n",
        "                if apply_log_transform and df[col].max() > 1000:  # Apply log transform to large values\n",
        "                    df[col] = np.log1p(df[col].clip(lower=0))\n",
        "\n",
        "        # Store post-imputation DataFrame\n",
        "        post_imputation_dfs[key] = df\n",
        "\n",
        "    e_ix = index_df[index_df[\"location_key\"].isin(common_keys)]\n",
        "    return pre_imputation_dfs, post_imputation_dfs, e_ix\n",
        "\n",
        "def exploratory_data_analysis(data_files, index_file_path='../data/index.csv', apply_log_transform=False):\n",
        "\n",
        "    pre_imputation_dfs, post_imputation_dfs, e_ix = load_and_clean_data(data_files, index_file_path, apply_log_transform)\n",
        "    if pre_imputation_dfs is None or post_imputation_dfs is None or e_ix is None:\n",
        "        print(\"Failed to load data. Exiting analysis.\")\n",
        "        return\n",
        "\n",
        "    # Compare all DataFrames (pre- and post-imputation)\n",
        "    comparison_data = []\n",
        "    for dataset_name in pre_imputation_dfs:\n",
        "        pre_df = pre_imputation_dfs[dataset_name]\n",
        "        post_df = post_imputation_dfs[dataset_name]\n",
        "\n",
        "        # Pre-imputation stats\n",
        "        pre_df_numeric = pre_df.drop(columns=['location_key', 'country_name', 'Time', 'Time Code'], errors='ignore')\n",
        "        if not pre_df_numeric.empty:\n",
        "            # Ensure numeric conversion for pre-imputation data\n",
        "            for col in pre_df_numeric.columns:\n",
        "                pre_df_numeric[col] = pd.to_numeric(pre_df_numeric[col], errors='coerce')\n",
        "\n",
        "            num_rows, num_cols = pre_df.shape\n",
        "            num_numeric_cols = len(pre_df_numeric.columns)\n",
        "            total_cells = num_rows * num_numeric_cols  # Count only numeric columns\n",
        "            missing_cells = pre_df_numeric.isna().sum().sum()  # Missing values in numeric columns\n",
        "            missing_percent = (missing_cells / total_cells * 100) if total_cells > 0 else 0\n",
        "            mean_variance = pre_df_numeric.var().mean() if not pre_df_numeric.empty and pre_df_numeric.var().notna().any() else 0\n",
        "            mean_unique = pre_df_numeric.nunique().mean() if not pre_df_numeric.empty else 0\n",
        "\n",
        "            if apply_log_transform:\n",
        "                for col in pre_df_numeric.columns:\n",
        "                    if pre_df_numeric[col].max() > 1000:\n",
        "                        pre_df_numeric[col] = np.log1p(pre_df_numeric[col].clip(lower=0))\n",
        "                mean_variance = pre_df_numeric.var().mean() if not pre_df_numeric.empty and pre_df_numeric.var().notna().any() else 0\n",
        "\n",
        "            comparison_data.append({\n",
        "                'Dataset': f\"{dataset_name} (Pre-Imputation)\",\n",
        "                'Rows': num_rows,\n",
        "                'Columns': num_cols,\n",
        "                'Numeric Columns': num_numeric_cols,\n",
        "                'Missing Values (%)': round(missing_percent, 2),\n",
        "                'Mean Variance': f\"{mean_variance:.2e}\" if mean_variance > 0 else \"0\",\n",
        "                'Mean Unique Values': round(mean_unique, 2)\n",
        "            })\n",
        "\n",
        "        # Post-imputation stats\n",
        "        post_df_numeric = post_df.drop(columns=['location_key', 'country_name', 'Time', 'Time Code'], errors='ignore')\n",
        "        if not post_df_numeric.empty:\n",
        "            num_rows, num_cols = post_df.shape\n",
        "            num_numeric_cols = len(post_df_numeric.columns)\n",
        "            total_cells = num_rows * num_numeric_cols  # Count only numeric columns\n",
        "            missing_cells = post_df_numeric.isna().sum().sum()  # Missing values in numeric columns\n",
        "            missing_percent = (missing_cells / total_cells * 100) if total_cells > 0 else 0\n",
        "            mean_variance = post_df_numeric.var().mean() if not post_df_numeric.empty else 0\n",
        "            mean_unique = post_df_numeric.nunique().mean() if not post_df_numeric.empty else 0\n",
        "\n",
        "            comparison_data.append({\n",
        "                'Dataset': f\"{dataset_name} (Post-Imputation)\",\n",
        "                'Rows': num_rows,\n",
        "                'Columns': num_cols,\n",
        "                'Numeric Columns': num_numeric_cols,\n",
        "                'Missing Values (%)': round(missing_percent, 2),\n",
        "                'Mean Variance': f\"{mean_variance:.2e}\" if mean_variance > 0 else \"0\",\n",
        "                'Mean Unique Values': round(mean_unique, 2)\n",
        "            })\n",
        "\n",
        "    # Create and display comparison DataFrame\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    if not comparison_df.empty:\n",
        "        display(HTML(\"<h2>Comparison of All Datasets (Pre- and Post-Imputation)</h2>\"))\n",
        "        display(HTML(comparison_df.style.set_table_styles([\n",
        "            {'selector': 'th', 'props': [('background-color', '#4CAF50'), ('color', 'white'), ('font-weight', 'bold'), ('text-align', 'center'), ('padding', '8px')]},\n",
        "            {'selector': 'td', 'props': [('border', '1px solid #ddd'), ('padding', '8px'), ('text-align', 'center'), ('color', 'black')]},\n",
        "            {'selector': 'tr:nth-child(even)', 'props': [('background-color', '#f2f2f2')]},\n",
        "            {'selector': 'tr:nth-child(odd)', 'props': [('background-color', 'white')]},\n",
        "            {'selector': 'tr:hover', 'props': [('background-color', '#ddd')]}\n",
        "        ]).set_caption(\"Dataset Comparison (Pre- and Post-Imputation)\").format(precision=2).hide(axis='index').to_html()))\n",
        "\n",
        "    # Individual dataset analysis (post-imputation only)\n",
        "    for dataset_name, df in post_imputation_dfs.items():\n",
        "        print(f\"\\n--- Exploratory Data Analysis for {dataset_name} (Post-Imputation) ---\")\n",
        "\n",
        "        # Prepare data by dropping non-numeric columns\n",
        "        df_numeric = df.drop(columns=['location_key', 'country_name', 'Time', 'Time Code'], errors='ignore')\n",
        "        if df_numeric.empty:\n",
        "            print(f\"No numeric columns in {dataset_name}. Skipping analysis.\")\n",
        "            continue\n",
        "\n",
        "        # 1. Basic Statistics\n",
        "        stats = df_numeric.describe().T[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
        "        stats = stats.round(4)\n",
        "        stats.index.name = 'Feature'\n",
        "        stats.reset_index(inplace=True)\n",
        "\n",
        "        # 2. Missing Values\n",
        "        missing = df_numeric.isna().sum().to_frame(name='Missing Values')\n",
        "        missing['Missing %'] = (missing['Missing Values'] / len(df_numeric) * 100).round(2)\n",
        "        missing = missing[missing['Missing Values'] > 0]\n",
        "        missing.index.name = 'Feature'\n",
        "        missing.reset_index(inplace=True)\n",
        "\n",
        "        # 3. Correlation Analysis (Top 5 correlations)\n",
        "        corr_matrix = df_numeric.corr().abs()\n",
        "        corr_pairs = corr_matrix.unstack().sort_values(ascending=False)\n",
        "        corr_pairs = corr_pairs[corr_pairs < 1.0].drop_duplicates().head(5)\n",
        "        corr_df = pd.DataFrame({\n",
        "            'Feature 1': [pair[0] for pair in corr_pairs.index],\n",
        "            'Feature 2': [pair[1] for pair in corr_pairs.index],\n",
        "            'Correlation': corr_pairs.values.round(4)\n",
        "        })\n",
        "\n",
        "        # 4. Variance and Unique Values\n",
        "        variance = df_numeric.var().to_frame(name='Variance').round(4)\n",
        "        unique_counts = df_numeric.nunique().to_frame(name='Unique Values')\n",
        "        feature_stats = variance.join(unique_counts)\n",
        "        feature_stats.index.name = 'Feature'\n",
        "        feature_stats.reset_index(inplace=True)\n",
        "\n",
        "        # Styling function for tables\n",
        "        def style_table(df):\n",
        "            return df.style.set_table_styles([\n",
        "                {'selector': 'th', 'props': [('background-color', '#4CAF50'), ('color', 'white'), ('font-weight', 'bold'), ('text-align', 'center'), ('padding', '8px')]},\n",
        "                {'selector': 'td', 'props': [('border', '1px solid #ddd'), ('padding', '8px'), ('text-align', 'center'), ('color', 'black')]},\n",
        "                {'selector': 'tr:nth-child(even)', 'props': [('background-color', '#f2f2f2')]},\n",
        "                {'selector': 'tr:nth-child(odd)', 'props': [('background-color', 'white')]},\n",
        "                {'selector': 'tr:hover', 'props': [('background-color', '#ddd')]}\n",
        "            ]).set_caption(f\"{dataset_name.capitalize()} Analysis (Post-Imputation)\").format(precision=2).hide(axis='index').to_html()\n",
        "\n",
        "        # Display tables\n",
        "        # display(HTML(f\"<h3>Basic Statistics for {dataset_name} (Post-Imputation)</h3>\"))\n",
        "        # display(HTML(style_table(stats)))\n",
        "\n",
        "        if not missing.empty:\n",
        "            display(HTML(f\"<h3>Missing Values in {dataset_name} (Post-Imputation)</h3>\"))\n",
        "            display(HTML(style_table(missing)))\n",
        "\n",
        "        # display(HTML(f\"<h3>Top 5 Correlations in {dataset_name} (Post-Imputation)</h3>\"))\n",
        "        # display(HTML(style_table(corr_df)))\n",
        "\n",
        "        display(HTML(f\"<h3>Feature Variance and Unique Values in {dataset_name} (Post-Imputation)</h3>\"))\n",
        "        display(HTML(style_table(feature_stats)))\n",
        "\n",
        "\n",
        "\n",
        "data_files = {\n",
        "    \"wb_econ\": \"../data/wb_econ.csv\",\n",
        "    \"wb_env\": \"../data/wb_env.csv\",\n",
        "    \"wb_edu\": \"../data/wb_edu.csv\",\n",
        "    \"wb_fin\": \"../data/wb_fin.csv\",\n",
        "    \"wb_health\": \"../data/wb_health.csv\",\n",
        "    \"wb_private\": \"../data/wb_private.csv\",\n",
        "    \"wb_public\": \"../data/wb_public.csv\",\n",
        "    \"wb_soc\": \"../data/wb_soc.csv\"\n",
        "}\n",
        "exploratory_data_analysis(data_files, apply_log_transform=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06BRtD2gI91n"
      },
      "outputs": [],
      "source": [
        "def load_and_clean_data(data_files, index_file_path='index.csv', max_nan_percent=0.5):\n",
        "\n",
        "    try:\n",
        "        index_df = pd.read_csv(index_file_path, keep_default_na=False, na_values=[])\n",
        "        if 'iso_3166_1_alpha_3' not in index_df.columns or 'location_key' not in index_df.columns:\n",
        "            raise KeyError(\"Index file must contain 'iso_3166_1_alpha_3' and 'location_key' columns.\")\n",
        "        alpha3_to_alpha2_map = dict(zip(index_df['iso_3166_1_alpha_3'], index_df['location_key']))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Index file '{index_file_path}' not found!\")\n",
        "        return None, None\n",
        "\n",
        "    raw_data = {}\n",
        "    for key, file in data_files.items():\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df.replace(\"..\", np.nan, inplace=True)\n",
        "            df['Country Code'] = df['Country Code'].map(alpha3_to_alpha2_map)\n",
        "            df = df[df['Country Code'].notna()]\n",
        "            df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            if \"Country Code\" in df.columns:\n",
        "                df[\"location_key\"] = df[\"Country Code\"]\n",
        "                df.drop(columns=[\"Country Code\"], inplace=True)\n",
        "\n",
        "            if \"Country Name\" in df.columns:\n",
        "                df.rename(columns={\"Country Name\": \"country_name\"}, inplace=True)\n",
        "\n",
        "            raw_data[key] = df\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Data file '{file}' not found!\")\n",
        "            return None, None\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file '{file}': {e}\")\n",
        "            return None, None\n",
        "\n",
        "    common_keys = set(index_df[\"location_key\"].dropna().unique())\n",
        "    for df in raw_data.values():\n",
        "        if \"location_key\" in df.columns:\n",
        "            common_keys &= set(df[\"location_key\"].dropna().unique())\n",
        "\n",
        "    cleaned_dfs = {}\n",
        "    for key, df in raw_data.items():\n",
        "        if \"location_key\" not in df.columns:\n",
        "            print(f\"Error: {key} missing location_key!\")\n",
        "            continue\n",
        "\n",
        "        df = df[df[\"location_key\"].isin(common_keys)].copy()\n",
        "        max_nans = int(max_nan_percent * len(df))\n",
        "        nan_counts = df.isna().sum()\n",
        "        df = df.loc[:, nan_counts <= max_nans]\n",
        "\n",
        "        numeric_cols = [col for col in df.columns if col not in [\"location_key\", \"country_name\"]]\n",
        "        for col in numeric_cols:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "            if df[col].notna().any():\n",
        "                df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        cleaned_dfs[key] = df\n",
        "\n",
        "    e_ix = index_df[index_df[\"location_key\"].isin(common_keys)]\n",
        "    return cleaned_dfs, e_ix\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer, MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data(data, n_bins=5, encode='ordinal', strategy='quantile', corr_threshold=0.95, top_n=10):\n",
        "\n",
        "    data = data.copy()\n",
        "\n",
        "    # Usuwanie kolumn o stałych wartościach\n",
        "    nunique = data.nunique()\n",
        "    data.drop(columns=nunique[nunique <= 1].index, inplace=True)\n",
        "\n",
        "    # Konwersja na wartości numeryczne\n",
        "    for col in data.columns:\n",
        "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Wypełnianie brakujących wartości medianą\n",
        "    numeric_cols = data.select_dtypes(include=np.number).columns\n",
        "    for col in numeric_cols:\n",
        "        if data[col].notna().any():\n",
        "            data[col].fillna(data[col].median(), inplace=True)\n",
        "        else:\n",
        "            data.drop(columns=[col], inplace=True)\n",
        "\n",
        "    if data.empty or len(data.columns) == 0:\n",
        "        print(\"Warning: DataFrame is empty after initial cleaning. Cannot proceed with feature selection.\")\n",
        "        return np.array([]), [], pd.Series()\n",
        "\n",
        "    # Obliczanie wariancji przed skalowaniem\n",
        "    pre_scaling_variances = data.var()\n",
        "\n",
        "    # Usuwanie wysoko skorelowanych kolumn\n",
        "    if len(data.columns) > 1:\n",
        "        corr_matrix = data.corr().abs()\n",
        "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "        to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column] > corr_threshold)]\n",
        "        data.drop(columns=to_drop_corr, inplace=True)\n",
        "        pre_scaling_variances = pre_scaling_variances.drop(to_drop_corr, errors='ignore')\n",
        "\n",
        "    if data.empty or len(data.columns) == 0:\n",
        "        print(\"Warning: DataFrame is empty after correlation filtering. Cannot proceed with feature selection.\")\n",
        "        return np.array([]), [], pd.Series()\n",
        "\n",
        "    # Dyskretyzacja\n",
        "    adjusted_n_bins = {}\n",
        "    for col in data.columns:\n",
        "        unique_vals = data[col].nunique()\n",
        "        adjusted_n_bins[col] = min(n_bins, unique_vals)\n",
        "        if adjusted_n_bins[col] < 2:\n",
        "            data.drop(columns=[col], inplace=True)\n",
        "            pre_scaling_variances = pre_scaling_variances.drop(col, errors='ignore')\n",
        "            print(f\"Warning: Dropping column '{col}' as it has fewer than 2 unique values for binning.\")\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"Warning: DataFrame is empty after binning adjustment. Cannot proceed with scaling.\")\n",
        "        return np.array([]), [], pd.Series()\n",
        "\n",
        "    kb = KBinsDiscretizer(n_bins=n_bins, encode=encode, strategy=strategy)\n",
        "    try:\n",
        "        data_binned = pd.DataFrame(kb.fit_transform(data), columns=data.columns, index=data.index)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error during KBinsDiscretizer: {e}. Returning empty array.\")\n",
        "        return np.array([]), [], pd.Series()\n",
        "\n",
        "    # Skalowanie MinMax\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data_binned), columns=data.columns, index=data.index)\n",
        "\n",
        "    # Wybór cech na podstawie wariancji po skalowaniu\n",
        "    if len(data_scaled.columns) == 0:\n",
        "        print(\"Warning: No columns available for variance calculation.\")\n",
        "        return np.array([]), [], pd.Series()\n",
        "    elif len(data_scaled.columns) == 1:\n",
        "        print(\"Warning: Only one feature available. Selecting it directly.\")\n",
        "        top_features = list(data_scaled.columns)\n",
        "    else:\n",
        "        variances = data_scaled.var()\n",
        "        if len(variances) == 0:\n",
        "            print(\"Warning: No features with variance to select from.\")\n",
        "            return np.array([]), [], pd.Series()\n",
        "        top_features = variances.sort_values(ascending=False).head(top_n).index.tolist()\n",
        "\n",
        "    # Debug: Sprawdzenie liczby wybranych kolumn i ich nazw\n",
        "    print(f\"DEBUG: Selected {len(top_features)} columns: {top_features}\")\n",
        "    if len(top_features) != top_n:\n",
        "        print(f\"Warning: Expected {top_n} features, but selected {len(top_features)}.\")\n",
        "\n",
        "    # Przycięcie danych do wybranych cech\n",
        "    data_scaled = data_scaled[top_features].to_numpy()\n",
        "\n",
        "    return data_scaled, top_features, pre_scaling_variances\n",
        "\n",
        "def exploratory_data_analysis(data_files, index_file_path='../data/index.csv'):\n",
        "\n",
        "    cleaned_dfs, e_ix = load_and_clean_data(data_files, index_file_path)\n",
        "    if cleaned_dfs is None or e_ix is None:\n",
        "        print(\"Failed to load data. Exiting analysis.\")\n",
        "        return\n",
        "\n",
        "    for dataset_name, df in cleaned_dfs.items():\n",
        "        print(f\"\\n--- Exploratory Data Analysis for {dataset_name} ---\")\n",
        "\n",
        "        # Prepare data by dropping non-numeric columns\n",
        "        df_no_key = df.drop(columns=['location_key', 'country_name', 'Time', 'Time Code'], errors='ignore')\n",
        "        if df_no_key.empty:\n",
        "            print(f\"No numeric columns in {dataset_name}. Skipping analysis.\")\n",
        "            continue\n",
        "\n",
        "        # Apply preprocessing to select features\n",
        "        _, selected_features, pre_scaling_variances = preprocess_data(df_no_key)\n",
        "        if not selected_features:\n",
        "            print(f\"No features selected after preprocessing for {dataset_name}. Skipping analysis.\")\n",
        "            continue\n",
        "        df_numeric = df_no_key[selected_features]\n",
        "\n",
        "        # Obliczanie wariancji po skalowaniu\n",
        "        kb = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
        "        data_binned = pd.DataFrame(kb.fit_transform(df_numeric), columns=df_numeric.columns, index=df_numeric.index)\n",
        "        scaler = MinMaxScaler()\n",
        "        data_scaled = pd.DataFrame(scaler.fit_transform(data_binned), columns=df_numeric.columns, index=df_numeric.index)\n",
        "        post_scaling_variances = data_scaled.var()\n",
        "\n",
        "        # 1. Basic Statistics\n",
        "        stats = df_numeric.describe().T[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
        "        stats = stats.round(4)\n",
        "        stats.index.name = 'Feature'\n",
        "        stats.reset_index(inplace=True)\n",
        "\n",
        "        # 2. Missing Values\n",
        "        missing = df_numeric.isna().sum().to_frame(name='Missing Values')\n",
        "        missing['Missing %'] = (missing['Missing Values'] / len(df_numeric) * 100).round(2)\n",
        "        missing = missing[missing['Missing Values'] > 0]\n",
        "        missing.index.name = 'Feature'\n",
        "        missing.reset_index(inplace=True)\n",
        "\n",
        "        # 3. Correlation Analysis (Top 5 correlations)\n",
        "        corr_matrix = df_numeric.corr().abs()\n",
        "        corr_pairs = corr_matrix.unstack().sort_values(ascending=False)\n",
        "        corr_pairs = corr_pairs[corr_pairs < 1.0].drop_duplicates().head(5)\n",
        "        corr_df = pd.DataFrame({\n",
        "            'Feature 1': [pair[0] for pair in corr_pairs.index],\n",
        "            'Feature 2': [pair[1] for pair in corr_pairs.index],\n",
        "            'Correlation': corr_pairs.values.round(4)\n",
        "        })\n",
        "\n",
        "        # 4. Variance and Unique Values\n",
        "        feature_stats = pd.DataFrame({\n",
        "            'Feature': selected_features,\n",
        "            'Variance (Pre-Scaling)': pre_scaling_variances[selected_features].round(4),\n",
        "            'Variance (Post-Scaling)': post_scaling_variances[selected_features].round(4),\n",
        "            'Unique Values': df_numeric.nunique()\n",
        "        }).reset_index(drop=True)\n",
        "\n",
        "        # Styling function for tables\n",
        "        def style_table(df):\n",
        "            return df.style.set_table_styles([\n",
        "                {'selector': 'th', 'props': [('background-color', '#4CAF50'), ('color', 'white'), ('font-weight', 'bold'), ('text-align', 'center'), ('padding', '8px')]},\n",
        "                {'selector': 'td', 'props': [('border', '1px solid #ddd'), ('padding', '8px'), ('text-align', 'center'), ('color', 'black')]},\n",
        "                {'selector': 'tr:nth-child(even)', 'props': [('background-color', '#f2f2f2')]},\n",
        "                {'selector': 'tr:nth-child(odd)', 'props': [('background-color', 'white')]},\n",
        "                {'selector': 'tr:hover', 'props': [('background-color', '#ddd')]}\n",
        "            ]).set_caption(f\"{dataset_name.capitalize()} Analysis (Selected Features)\").format(precision=4).to_html()\n",
        "\n",
        "        # Display tables\n",
        "        if not missing.empty:\n",
        "            display(HTML(f\"<h3>Missing Values in {dataset_name} (Selected Features)</h3>\"))\n",
        "            display(HTML(style_table(missing)))\n",
        "\n",
        "        display(HTML(f\"<h3>Feature Variance and Unique Values in {dataset_name} (Selected Features)</h3>\"))\n",
        "        display(HTML(style_table(feature_stats)))\n",
        "\n",
        "\n",
        "exploratory_data_analysis(data_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJfWNzfF2Xjt"
      },
      "source": [
        "## 3. Przetwarzanie wstępne i optymalizacja grupowania\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr7aX0tI18Xi"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data, n_bins=5, encode='ordinal', strategy='quantile', corr_threshold=0.95, top_n=10):\n",
        "    data = data.copy()\n",
        "\n",
        "    nunique = data.nunique()\n",
        "    data.drop(columns=nunique[nunique <= 1].index, inplace=True)\n",
        "\n",
        "    for col in data.columns:\n",
        "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    numeric_cols = data.select_dtypes(include=np.number).columns\n",
        "    for col in numeric_cols:\n",
        "        if data[col].notna().sum() > 0:\n",
        "            data[col].fillna(data[col].median(), inplace=True)\n",
        "        else:\n",
        "            data.drop(columns=[col], inplace=True)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"Warning: DataFrame is empty after initial cleaning.\")\n",
        "        return np.array([]), []\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
        "\n",
        "    if len(data_scaled.columns) > 1:\n",
        "        corr_matrix = data_scaled.corr().abs()\n",
        "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "        to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > corr_threshold)]\n",
        "        data_scaled.drop(columns=to_drop, inplace=True)\n",
        "\n",
        "    if data_scaled.empty:\n",
        "        print(\"Warning: DataFrame is empty after correlation filtering.\")\n",
        "        return np.array([]), []\n",
        "\n",
        "    data_binned = pd.DataFrame(index=data_scaled.index, columns=data_scaled.columns)\n",
        "    adjusted_n_bins = {}\n",
        "    for col in data_scaled.columns:\n",
        "        unique_vals = data[col].nunique()\n",
        "        adjusted_n_bins[col] = min(n_bins, unique_vals)\n",
        "        if adjusted_n_bins[col] < 2:\n",
        "            print(f\"Warning: Dropping column '{col}' as it has fewer than 2 unique values for binning.\")\n",
        "            continue\n",
        "\n",
        "        kb = KBinsDiscretizer(n_bins=adjusted_n_bins[col], encode=encode, strategy=strategy)\n",
        "        try:\n",
        "            binned_col = kb.fit_transform(data[[col]])\n",
        "            data_binned[col] = binned_col.ravel()\n",
        "        except ValueError as e:\n",
        "            print(f\"Error during KBinsDiscretizer for column '{col}': {e}. Dropping column.\")\n",
        "            continue\n",
        "\n",
        "    variances = data_binned.var()\n",
        "    if not variances.empty:\n",
        "        top_features = variances.sort_values(ascending=False).head(top_n).index.tolist()\n",
        "        data_binned = data_binned[top_features]\n",
        "    else:\n",
        "        print(\"Warning: No features with variance.\")\n",
        "        return np.array([]), []\n",
        "\n",
        "\n",
        "    final_scaler = MinMaxScaler()\n",
        "    data_final = final_scaler.fit_transform(data_binned)\n",
        "\n",
        "    return data_final, list(data_binned.columns)\n",
        "\n",
        "\n",
        "\n",
        "def suggest_n_clusters(data_scaled, dataset_name):\n",
        "    if data_scaled.shape[0] == 0 or data_scaled.shape[1] == 0:\n",
        "        print(f\"No data for {dataset_name}. Defaulting to 3 clusters.\")\n",
        "        return 3\n",
        "\n",
        "    svd = plot_svd_visualization(data_scaled, dataset_name)\n",
        "    if not hasattr(svd, 'explained_variance_ratio_'):\n",
        "        return 3\n",
        "    cumulative_variance = np.cumsum(svd.explained_variance_ratio_)\n",
        "    n_clusters = np.argmax(cumulative_variance >= 0.9) if np.any(cumulative_variance >= 0.8) else 3\n",
        "    return max(2, min(n_clusters, 10))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnW17qy12Zi1"
      },
      "source": [
        "## 4. Redukcja wymiarowości (t-SNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExnHpKDv191O"
      },
      "outputs": [],
      "source": [
        "def reduce_dimensions(data_scaled, n_samples):\n",
        "    if n_samples < 2:\n",
        "        print(\"Warning: Not enough samples for t-SNE.\")\n",
        "        if data_scaled.shape[1] == 2:\n",
        "            return data_scaled\n",
        "        return np.array([])\n",
        "\n",
        "    perplexity = min(30, max(5, n_samples // 3))\n",
        "    n_features = data_scaled.shape[1]\n",
        "    if n_features < 2:\n",
        "        print(f\"Warning: Not enough features ({n_features}) for 2D t-SNE.\")\n",
        "        if n_features == 1:\n",
        "            return np.c_[data_scaled, np.zeros(len(data_scaled))]\n",
        "        return np.array([])\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate='auto', n_iter=1000, random_state=42)\n",
        "    return tsne.fit_transform(data_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCW_fLJJ2h0J"
      },
      "source": [
        "## 5. Grupowanie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SHQd3KB1_S-"
      },
      "outputs": [],
      "source": [
        "def hierarchical_clustering(data_scaled, n_clusters):\n",
        "    if data_scaled.shape[0] < 2 or data_scaled.shape[1] < 1:\n",
        "        print(\"Warning: Insufficient data for hierarchical clustering. Returning empty clusters.\")\n",
        "        return np.array([0] * data_scaled.shape[0]), None\n",
        "\n",
        "    Z = linkage(data_scaled, method='ward', metric='euclidean')\n",
        "    clusters = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
        "    return clusters - 1, Z\n",
        "\n",
        "\n",
        "def dbscan_clustering(data_scaled):\n",
        "    if data_scaled.shape[0] < 2 or data_scaled.shape[1] < 1:\n",
        "        print(\"Warning: Insufficient data for DBSCAN clustering. Returning empty clusters.\")\n",
        "        return np.array([0] * data_scaled.shape[0])\n",
        "\n",
        "    min_samples = min(4, data_scaled.shape[0] - 1)\n",
        "    neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
        "    neighbors_fit = neighbors.fit(data_scaled)\n",
        "    distances, _ = neighbors_fit.kneighbors(data_scaled)\n",
        "    distances = np.sort(distances[:, min_samples-1], axis=0)\n",
        "    eps = np.percentile(distances, 75)\n",
        "\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
        "    clusters = dbscan.fit_predict(data_scaled)\n",
        "    return clusters\n",
        "\n",
        "def cluster_data(data_2d, n_clusters, clustering_method='kmeans'):\n",
        "    if data_2d.shape[0] < 2:\n",
        "        print(\"Warning: Insufficient data for clustering.\")\n",
        "        return np.array([0] * data_2d.shape[0]), None, None\n",
        "\n",
        "    if clustering_method == 'kmeans':\n",
        "        if n_clusters < 1 or data_2d.shape[0] < n_clusters:\n",
        "            print(\"Warning: Invalid n_clusters for K-Means.\")\n",
        "            return np.array([0] * data_2d.shape[0]), np.zeros((1, 2)), None\n",
        "        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, random_state=42)\n",
        "        clusters = kmeans.fit_predict(data_2d)\n",
        "        return clusters, kmeans.cluster_centers_, None\n",
        "    elif clustering_method == 'hierarchical':\n",
        "        clusters, Z = hierarchical_clustering(data_2d, n_clusters)\n",
        "        return clusters, None, Z\n",
        "    elif clustering_method == 'dbscan':\n",
        "        clusters = dbscan_clustering(data_2d)\n",
        "        return clusters, None, None\n",
        "    else:\n",
        "        print(f\"Unknown clustering method: {clustering_method}. Using K-means.\")\n",
        "        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, random_state=42)\n",
        "        clusters = kmeans.fit_predict(data_2d)\n",
        "        return clusters, kmeans.cluster_centers_, None\n",
        "\n",
        "def find_optimal_n_bins(df_no_key, n_samples, n_clusters, n_bins_range=[3, 5, 7, 10], dataset_name=\"\", clustering_method='kmeans'):\n",
        "    silhouette_scores = []\n",
        "    best_n_bins = n_bins_range[0]\n",
        "    best_score = -1\n",
        "    best_clusters = None\n",
        "    best_data_2d = None\n",
        "\n",
        "    if df_no_key.empty or n_samples < 2:\n",
        "        print(f\"No data for {dataset_name}. Skipping bin optimization.\")\n",
        "        return best_n_bins, np.zeros(n_samples), np.zeros((n_samples, 2))\n",
        "\n",
        "    df_clean = df_no_key.loc[:, df_no_key.nunique() > 1]\n",
        "    if df_clean.empty:\n",
        "        print(f\"No valid columns in {dataset_name}.\")\n",
        "        return best_n_bins, np.zeros(n_samples), np.zeros((n_samples, 2))\n",
        "\n",
        "    for n_bins in n_bins_range:\n",
        "        data_scaled, _ = preprocess_data(df_clean, n_bins=n_bins)\n",
        "        if data_scaled.shape[0] == 0:\n",
        "            silhouette_scores.append(-1)\n",
        "            continue\n",
        "\n",
        "        data_2d = reduce_dimensions(data_scaled, n_samples)\n",
        "        clusters, _, _ = cluster_data(data_2d, n_clusters, clustering_method)\n",
        "\n",
        "        silhouette = -1 if len(set(clusters)) <= 1 else silhouette_score(data_2d, clusters)\n",
        "        silhouette_scores.append(silhouette)\n",
        "\n",
        "        if silhouette > best_score:\n",
        "            best_score = silhouette\n",
        "            best_n_bins = n_bins\n",
        "            best_clusters = clusters\n",
        "            best_data_2d = data_2d\n",
        "\n",
        "    if best_clusters is None:\n",
        "        data_scaled, _ = preprocess_data(df_clean, n_bins=best_n_bins)\n",
        "        best_data_2d = reduce_dimensions(data_scaled, n_samples)\n",
        "        best_clusters, _, _ = cluster_data(best_data_2d, n_clusters, clustering_method)\n",
        "\n",
        "    plot_kbins_visualization(df_clean, silhouette_scores, n_bins_range, dataset_name)\n",
        "    return best_n_bins, best_clusters, best_data_2d\n",
        "\n",
        "\n",
        "def compute_evaluation_metrics(data_2d, clusters, cluster_centers=None, clustering_method='kmeans'):\n",
        "    # Obliczanie Silhouette Score\n",
        "    silhouette = silhouette_score(data_2d, clusters) if len(set(clusters)) > 1 else -1\n",
        "\n",
        "    # Obliczanie centroidów, jeśli nie są dostarczone\n",
        "    if cluster_centers is None:\n",
        "        unique_clusters = np.unique(clusters)\n",
        "        # Pomijamy szum (-1) w DBSCAN przy obliczaniu centroidów\n",
        "        valid_clusters = [c for c in unique_clusters if c != -1] if clustering_method == 'dbscan' else unique_clusters\n",
        "        if len(valid_clusters) == 0:\n",
        "            return silhouette, 0, 0, (clusters == -1).sum() / len(clusters) if clustering_method == 'dbscan' else None\n",
        "\n",
        "        cluster_centers = np.zeros((len(valid_clusters), data_2d.shape[1]))\n",
        "        for i, cluster_id in enumerate(valid_clusters):\n",
        "            cluster_points = data_2d[clusters == cluster_id]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_centers[i] = np.mean(cluster_points, axis=0)\n",
        "            else:\n",
        "                cluster_centers[i] = np.zeros(data_2d.shape[1])\n",
        "\n",
        "        cluster_id_to_center = {cid: center for cid, center in zip(valid_clusters, cluster_centers)}\n",
        "    else:\n",
        "        cluster_id_to_center = {i: center for i, center in enumerate(cluster_centers)}\n",
        "\n",
        "    # Obliczanie WC Distance (średnia odległość punktów od centroidów ich klastrów)\n",
        "    wc_distances = []\n",
        "    for idx, point in enumerate(data_2d):\n",
        "        cluster_id = clusters[idx]\n",
        "        # Pomijamy punkty szumowe w DBSCAN\n",
        "        if clustering_method == 'dbscan' and cluster_id == -1:\n",
        "            continue\n",
        "        if cluster_id in cluster_id_to_center:\n",
        "            wc_distances.append(euclidean(point, cluster_id_to_center[cluster_id]))\n",
        "    wc_distance = np.mean(wc_distances) if wc_distances else 0\n",
        "\n",
        "    # Obliczanie BC Distance (średnia odległość między centroidami)\n",
        "    bc_distances = []\n",
        "    valid_clusters = list(cluster_id_to_center.keys())\n",
        "    if len(valid_clusters) > 1:\n",
        "        for i in range(len(valid_clusters)):\n",
        "            for j in range(i + 1, len(valid_clusters)):\n",
        "                c1_id, c2_id = valid_clusters[i], valid_clusters[j]\n",
        "                bc_distances.append(euclidean(cluster_id_to_center[c1_id], cluster_id_to_center[c2_id]))\n",
        "    bc_distance = np.mean(bc_distances) if bc_distances else 0\n",
        "\n",
        "    # Obliczanie Noise Ratio dla DBSCAN\n",
        "    noise_ratio = (clusters == -1).sum() / len(clusters) if clustering_method == 'dbscan' else None\n",
        "\n",
        "    return silhouette, wc_distance, bc_distance, noise_ratio\n",
        "\n",
        "def analyze_dataset(df, name, e_ix, clustering_method='kmeans', n_bins_range=[3, 5, 7, 10]):\n",
        "    location_keys = df['location_key'].tolist()\n",
        "    df_no_key = df.drop(columns=['location_key', 'country_name', 'Time', 'Time Code'], errors='ignore')\n",
        "\n",
        "    if df_no_key.shape[1] < 2:\n",
        "        print(f\"Warning: Not enough features in {name} for clustering.\")\n",
        "        return pd.DataFrame({\"location_key\": location_keys, f\"Cluster_{name}\": [0]*len(location_keys)}), -1, 0, 0, [], None\n",
        "\n",
        "    temp_scaled, initial_features = preprocess_data(df_no_key)\n",
        "    n_clusters = suggest_n_clusters(temp_scaled, name) if clustering_method != 'dbscan' else 3\n",
        "\n",
        "    best_n_bins, best_clusters, best_data_2d = find_optimal_n_bins(\n",
        "        df_no_key, len(df_no_key), n_clusters, n_bins_range, name, clustering_method\n",
        "    )\n",
        "\n",
        "    data_scaled, selected_features = preprocess_data(df_no_key, n_bins=best_n_bins)\n",
        "    print(f\"Selected {len(selected_features)} features for {name}: {selected_features}\")\n",
        "    data_2d = reduce_dimensions(data_scaled, len(df_no_key))\n",
        "\n",
        "    clusters, centers, Z = cluster_data(data_2d, n_clusters, clustering_method)\n",
        "\n",
        "    # Obliczanie centroidów dla metody hierarchicznej\n",
        "    if clustering_method == 'hierarchical' and centers is None:\n",
        "        unique_clusters = np.unique(clusters)\n",
        "        centers = np.zeros((len(unique_clusters), data_2d.shape[1]))\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            cluster_points = data_2d[clusters == cluster_id]\n",
        "            if len(cluster_points) > 0:\n",
        "                centers[i] = np.mean(cluster_points, axis=0)\n",
        "            else:\n",
        "                centers[i] = np.zeros(data_2d.shape[1])\n",
        "\n",
        "    # Wizualizacja\n",
        "    if clustering_method == 'kmeans':\n",
        "        plot_interactive_clusters_graph(data_2d, clusters, location_keys, e_ix, f\"{name} (KMeans)\")\n",
        "    elif clustering_method == 'hierarchical':\n",
        "        if Z is not None:\n",
        "            plot_dendrogram(Z, location_keys, e_ix, name)\n",
        "        # Dodajemy wykres 2D analogiczny do K-Means\n",
        "        plot_interactive_clusters_graph(data_2d, clusters, location_keys, e_ix, f\"{name} (Hierarchical)\")\n",
        "    elif clustering_method == 'dbscan':\n",
        "        plot_dbscan_interactive(data_2d, clusters, location_keys, e_ix, name)\n",
        "\n",
        "    silhouette, wc_distance, bc_distance, noise_ratio = compute_evaluation_metrics(data_2d, clusters, centers, clustering_method)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"location_key\": location_keys,\n",
        "        f\"Cluster_{name}\": clusters\n",
        "    }), silhouette, wc_distance, bc_distance, selected_features, noise_ratio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UosrKMk72p5v"
      },
      "source": [
        "## Wykonanie całego procesu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "DWhtC5B12DpH",
        "outputId": "3d9466bf-e50b-4430-ee4e-95b5e3bb9f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Index file 'data/index.csv' not found!\n",
            "\n",
            "--- NMI Matrices for Each Clustering Method ---\n",
            "\n",
            "=== NMI Matrix for Kmeans ===\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'keys'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== NMI Matrix for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Tworzenie macierzy NMI dla danej metody\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m dataset_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mcleaned_dfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n\u001b[0;32m     65\u001b[0m nmi_matrix \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m     66\u001b[0m     index\u001b[38;5;241m=\u001b[39mdataset_names,\n\u001b[0;32m     67\u001b[0m     columns\u001b[38;5;241m=\u001b[39mdataset_names,\n\u001b[0;32m     68\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m\n\u001b[0;32m     69\u001b[0m )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Obliczanie NMI dla każdej pary zbiorów danych w ramach danej metody\u001b[39;00m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
          ]
        }
      ],
      "source": [
        "cleaned_dfs, e_ix = load_and_clean_data(data_files, max_nan_percent=0.5)\n",
        "\n",
        "all_selected_features = {}\n",
        "if cleaned_dfs and e_ix is not None:\n",
        "    print(\"\\nLoaded datasets:\", cleaned_dfs.keys())\n",
        "    #for key, df in cleaned_dfs.items():\n",
        "        #print(f\"\\nHead for {key}:\\n{df.head()}\")\n",
        "        #print(f\"\\nNaN analysis for {key} (Shape: {df.shape}):\")\n",
        "        #nan_counts = df.isna().sum()\n",
        "        #nan_percent = df.isna().mean() * 100\n",
        "        #for col in df.columns:\n",
        "        #    print(f\"  {col}: {nan_counts[col]} NaN ({nan_percent[col]:.2f}%)\")\n",
        "\n",
        "    results = {}\n",
        "    evaluation_metrics = []\n",
        "    for clustering_method in ['kmeans', 'hierarchical', 'dbscan']:\n",
        "        print(f\"\\n=== Analyzing with {clustering_method.capitalize()} ===\")\n",
        "        for name, df in cleaned_dfs.items():\n",
        "            result_df, silhouette, wc_distance, bc_distance, selected_features, noise_ratio = analyze_dataset(\n",
        "                df, name, e_ix, clustering_method=clustering_method\n",
        "            )\n",
        "            results[(name, clustering_method)] = result_df\n",
        "            all_selected_features[name] = selected_features\n",
        "            metrics = {\n",
        "                \"Dataset\": name,\n",
        "                \"Method\": clustering_method,\n",
        "                \"Silhouette Score\": silhouette,\n",
        "                \"WC Distance\": wc_distance,\n",
        "                \"BC Distance\": bc_distance\n",
        "            }\n",
        "            if noise_ratio is not None:\n",
        "                metrics[\"Noise Ratio\"] = noise_ratio\n",
        "            evaluation_metrics.append(metrics)\n",
        "\n",
        "    print(\"\\n--- Evaluation Metrics ---\")\n",
        "    evaluation_df = pd.DataFrame(evaluation_metrics)\n",
        "    display(evaluation_df.style.set_caption(\"Clustering Evaluation Metrics\").format({\n",
        "        \"Silhouette Score\": \"{:.4f}\",\n",
        "        \"WC Distance\": \"{:.4f}\",\n",
        "        \"BC Distance\": \"{:.4f}\",\n",
        "        \"Noise Ratio\": \"{:.4f}\" if \"Noise Ratio\" in evaluation_df.columns else None\n",
        "    }))\n",
        "\n",
        "# Po analizie zbiorów danych i zebraniu wyników w `results`\n",
        "\n",
        "print(\"\\n--- NMI Matrices for Each Clustering Method ---\")\n",
        "clustering_methods = ['kmeans', 'hierarchical', 'dbscan']\n",
        "\n",
        "for method in clustering_methods:\n",
        "    print(f\"\\n=== NMI Matrix for {method.capitalize()} ===\")\n",
        "    # Tworzenie macierzy NMI dla danej metody\n",
        "    dataset_names = list(cleaned_dfs.keys())\n",
        "    nmi_matrix = pd.DataFrame(\n",
        "        index=dataset_names,\n",
        "        columns=dataset_names,\n",
        "        dtype=float\n",
        "    )\n",
        "\n",
        "    # Obliczanie NMI dla każdej pary zbiorów danych w ramach danej metody\n",
        "    for name1 in dataset_names:\n",
        "        for name2 in dataset_names:\n",
        "            if name1 == name2:\n",
        "                nmi_matrix.loc[name1, name2] = 1.0\n",
        "            else:\n",
        "                # Pobieranie wyników klastrowania dla obu zbiorów danych\n",
        "                df1 = results.get((name1, method))\n",
        "                df2 = results.get((name2, method))\n",
        "\n",
        "                if df1 is not None and df2 is not None:\n",
        "                    merged_df = pd.merge(df1, df2, on='location_key', how='inner')\n",
        "                    if not merged_df.empty:\n",
        "                        cluster1_col = f\"Cluster_{name1}\"\n",
        "                        cluster2_col = f\"Cluster_{name2}\"\n",
        "                        try:\n",
        "                            nmi = normalized_mutual_info_score(\n",
        "                                merged_df[cluster1_col], merged_df[cluster2_col]\n",
        "                            )\n",
        "                            nmi_matrix.loc[name1, name2] = nmi\n",
        "                        except ValueError as e:\n",
        "                            print(f\"Error computing NMI for {name1} vs {name2} ({method}): {e}\")\n",
        "                            nmi_matrix.loc[name1, name2] = np.nan\n",
        "                    else:\n",
        "                        nmi_matrix.loc[name1, name2] = np.nan\n",
        "                else:\n",
        "                    nmi_matrix.loc[name1, name2] = np.nan\n",
        "\n",
        "    # Wizualizacja macierzy NMI\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(nmi_matrix, annot=True, cmap='viridis', fmt=\".3f\", linewidths=.5)\n",
        "    plt.title(f'NMI Matrix for {method.capitalize()} Clustering Across Datasets')\n",
        "    plt.xlabel('Datasets')\n",
        "    plt.ylabel('Datasets')\n",
        "    plt.show()\n",
        "\n",
        "    # Wyświetlenie macierzy w formie tekstowej\n",
        "    print(f\"\\nNMI Matrix for {method.capitalize()}:\\n\")\n",
        "    display(nmi_matrix.style.set_caption(f\"NMI Matrix for {method.capitalize()}\").format(\"{:.3f}\"))\n",
        "\n",
        "# NMI między metodami dla każdego datasetu\n",
        "print(\"\\n--- NMI Matrices for Each Dataset Across Clustering Methods ---\")\n",
        "for dataset_name in cleaned_dfs.keys():\n",
        "    print(f\"\\n=== NMI Matrix for {dataset_name.capitalize()} Across Clustering Methods ===\")\n",
        "    nmi_matrix = pd.DataFrame(\n",
        "        index=clustering_methods,\n",
        "        columns=clustering_methods,\n",
        "        dtype=float\n",
        "    )\n",
        "\n",
        "    for method1 in clustering_methods:\n",
        "        for method2 in clustering_methods:\n",
        "            if method1 == method2:\n",
        "                nmi_matrix.loc[method1, method2] = 1.0\n",
        "            else:\n",
        "                df1 = results.get((dataset_name, method1))\n",
        "                df2 = results.get((dataset_name, method2))\n",
        "                if df1 is not None and df2 is not None:\n",
        "                    merged_df = pd.merge(df1, df2, on='location_key', how='inner')\n",
        "                    if not merged_df.empty:\n",
        "                        cluster1_col = f\"Cluster_{dataset_name}\"\n",
        "                        cluster2_col = f\"Cluster_{dataset_name}\"\n",
        "                        try:\n",
        "                            nmi = normalized_mutual_info_score(\n",
        "                                merged_df[cluster1_col + '_x'], merged_df[cluster2_col + '_y']\n",
        "                            )\n",
        "                            nmi_matrix.loc[method1, method2] = nmi\n",
        "                        except ValueError as e:\n",
        "                            print(f\"Error computing NMI for {dataset_name} ({method1} vs {method2}): {e}\")\n",
        "                            nmi_matrix.loc[method1, method2] = np.nan\n",
        "                    else:\n",
        "                        nmi_matrix.loc[method1, method2] = np.nan\n",
        "                else:\n",
        "                    nmi_matrix.loc[method1, method2] = np.nan\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(nmi_matrix, annot=True, cmap='viridis', fmt=\".3f\", linewidths=.5)\n",
        "    plt.title(f'NMI Matrix for {dataset_name.capitalize()} Across Clustering Methods')\n",
        "    plt.xlabel('Methods')\n",
        "    plt.ylabel('Methods')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nNMI Matrix for {dataset_name.capitalize()} Across Clustering Methods:\\n\")\n",
        "    display(nmi_matrix.style.set_caption(f\"NMI Matrix for {dataset_name.capitalize()} Across Clustering Methods\").format(\"{:.3f}\"))\n",
        "\n",
        "    print(\"\\n--- Selected Features ---\")\n",
        "    for name, features in all_selected_features.items():\n",
        "        print(f\"**{name.capitalize()}:**\")\n",
        "        if features:\n",
        "            for feature in features:\n",
        "                print(f\"- {feature}\")\n",
        "        else:\n",
        "            print(\"- No features selected.\")\n",
        "        print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ed_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}